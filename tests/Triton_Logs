1001060000@triton-is-predictor-65dc6db5d7-nk9x5:/opt/tritonserver$ curl localhost:8002/metrics
# HELP nv_inference_request_success Number of successful inference requests, all batch sizes
# TYPE nv_inference_request_success counter
nv_inference_request_success{model="meta-llama-3-8b-instruct-awq",version="1"} 48
# HELP nv_inference_request_failure Number of failed inference requests, all batch sizes
# TYPE nv_inference_request_failure counter
nv_inference_request_failure{model="facebook-opt125m",reason="OTHER",version="1"} 0
nv_inference_request_failure{model="facebook-opt125m",reason="CANCELED",version="1"} 0
nv_inference_request_failure{model="facebook-opt125m",reason="REJECTED",version="1"} 0
nv_inference_request_failure{model="meta-llama-3-8b-instruct-awq",reason="BACKEND",version="1"} 0
nv_inference_request_failure{model="meta-llama-3-8b-instruct-awq",reason="CANCELED",version="1"} 0
nv_inference_request_failure{model="meta-llama-3-8b-instruct-awq",reason="OTHER",version="1"} 0
nv_inference_request_failure{model="meta-llama-3-8b-instruct-awq",reason="REJECTED",version="1"} 0
nv_inference_request_failure{model="facebook-opt125m",reason="BACKEND",version="1"} 0
nv_inference_request_failure{model="llama-3-8b-instruct",reason="OTHER",version="1"} 0
nv_inference_request_failure{model="llama-3-8b-instruct",reason="BACKEND",version="1"} 0
nv_inference_request_failure{model="llama-3-8b-instruct",reason="CANCELED",version="1"} 0
nv_inference_request_failure{model="llama-3-8b-instruct",reason="REJECTED",version="1"} 0
# HELP nv_inference_count Number of inferences performed (does not include cached requests)
# TYPE nv_inference_count counter
nv_inference_count{model="meta-llama-3-8b-instruct-awq",version="1"} 48
# HELP nv_inference_exec_count Number of model executions performed (does not include cached requests)
# TYPE nv_inference_exec_count counter
nv_inference_exec_count{model="meta-llama-3-8b-instruct-awq",version="1"} 48
# HELP nv_inference_request_duration_us Cumulative inference request duration in microseconds (includes cached requests)
# TYPE nv_inference_request_duration_us counter
nv_inference_request_duration_us{model="meta-llama-3-8b-instruct-awq",version="1"} 32889
# HELP nv_inference_queue_duration_us Cumulative inference queuing duration in microseconds (includes cached requests)
# TYPE nv_inference_queue_duration_us counter
nv_inference_queue_duration_us{model="meta-llama-3-8b-instruct-awq",version="1"} 3871
# HELP nv_inference_compute_input_duration_us Cumulative compute input duration in microseconds (does not include cached requests)
# TYPE nv_inference_compute_input_duration_us counter
nv_inference_compute_input_duration_us{model="meta-llama-3-8b-instruct-awq",version="1"} 2865
# HELP nv_inference_compute_infer_duration_us Cumulative compute inference duration in microseconds (does not include cached requests)
# TYPE nv_inference_compute_infer_duration_us counter
nv_inference_compute_infer_duration_us{model="meta-llama-3-8b-instruct-awq",version="1"} 25446
# HELP nv_inference_compute_output_duration_us Cumulative inference compute output duration in microseconds (does not include cached requests)
# TYPE nv_inference_compute_output_duration_us counter
nv_inference_compute_output_duration_us{model="meta-llama-3-8b-instruct-awq",version="1"} 381
# HELP nv_energy_consumption GPU energy consumption in joules since the Triton Server started
# TYPE nv_energy_consumption counter
nv_energy_consumption{gpu_uuid="GPU-47385698-1baf-7602-e480-a211177a1640"} 1769545.423999995
# HELP nv_inference_pending_request_count Instantaneous number of pending requests awaiting execution per-model.
# TYPE nv_inference_pending_request_count gauge
nv_inference_pending_request_count{model="meta-llama-3-8b-instruct-awq",version="1"} 0
# HELP nv_pinned_memory_pool_total_bytes Pinned memory pool total memory size, in bytes
# TYPE nv_pinned_memory_pool_total_bytes gauge
nv_pinned_memory_pool_total_bytes 268435456
# HELP nv_pinned_memory_pool_used_bytes Pinned memory pool used memory size, in bytes
# TYPE nv_pinned_memory_pool_used_bytes gauge
nv_pinned_memory_pool_used_bytes 0
# HELP nv_gpu_utilization GPU utilization rate [0.0 - 1.0)
# TYPE nv_gpu_utilization gauge
nv_gpu_utilization{gpu_uuid="GPU-47385698-1baf-7602-e480-a211177a1640"} 0
# HELP nv_gpu_memory_total_bytes GPU total memory, in bytes
# TYPE nv_gpu_memory_total_bytes gauge
nv_gpu_memory_total_bytes{gpu_uuid="GPU-47385698-1baf-7602-e480-a211177a1640"} 12884901888
# HELP nv_gpu_memory_used_bytes GPU used memory, in bytes
# TYPE nv_gpu_memory_used_bytes gauge
nv_gpu_memory_used_bytes{gpu_uuid="GPU-47385698-1baf-7602-e480-a211177a1640"} 9976152064
# HELP nv_gpu_power_usage GPU power usage in watts
# TYPE nv_gpu_power_usage gauge
nv_gpu_power_usage{gpu_uuid="GPU-47385698-1baf-7602-e480-a211177a1640"} 15.1
# HELP nv_gpu_power_limit GPU power management limit in watts
# TYPE nv_gpu_power_limit gauge
nv_gpu_power_limit{gpu_uuid="GPU-47385698-1baf-7602-e480-a211177a1640"} 170
# HELP nv_cpu_utilization CPU utilization rate [0.0 - 1.0]
# TYPE nv_cpu_utilization gauge
nv_cpu_utilization 0.2886334610472541
# HELP nv_cpu_memory_total_bytes CPU total memory (RAM), in bytes
# TYPE nv_cpu_memory_total_bytes gauge
nv_cpu_memory_total_bytes 101239308288
# HELP nv_cpu_memory_used_bytes CPU used memory (RAM), in bytes
# TYPE nv_cpu_memory_used_bytes gauge
nv_cpu_memory_used_bytes 56765489152