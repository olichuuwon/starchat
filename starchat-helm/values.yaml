# values.yaml

# Namespace for the resources
namespace: starchat

# Application settings for "application"
application:
  name: application
  image:
    repository: quay.io/jeslynlamxy/starchat-app
    tag: latest
  replicas: 1
  ports:
    containerPort: 8501 # Application's container port
  env:
    CLIENT_ID: oauth2-proxy
    CLIENT_SECRET: PylbLcV3Ru6tVq4BiSNcTiO7mcULvajP
    KEYCLOAK_URL: https://keycloak.apps.gemini.sl
    REALM: gemini
    REDIRECT_URI: https://application-starchat.apps.nebula.sl

    OLLAMA_MODEL_NAME: llama3:instruct
    OLLAMA_MODEL_BASE_URL: http://model:11434

    VLLM_MODEL_NAME: meta-llama-3-8b-instruct-awq
    VLLM_FULL_MODEL: http://triton-is-predictor.triton-inference-services.svc.cluster.local:8080/v2/models/meta-llama-3-8b-instruct-awq/generate
    
    LLM_PROVIDER: vllm
    AUTH_PROVIDER: oauth-proxy

    PROXY_JWT_KEY: _oauth2_proxy
    WEB_LINK: https://application-starchat.apps.nebula.sl

    LOGGING_ENDPOINT: postgresql+psycopg2://user:pass@logging:5432/logging

# Container resource settings (shared across applications)
resources:
  requests:
    cpu: "0.5"
    memory: "1Gi"
  limits:
    cpu: "1"
    memory: "2Gi"

# Route settings for "application" (OpenShift)
routeApplication:
  name: application
  tlsTermination: edge
  wildcardPolicy: None
