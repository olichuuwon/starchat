# values.yaml

# Namespace for the resources
namespace: starchat

# Application settings for "application"
application:
  name: application
  image:
    repository: quay.io/jeslynlamxy/starchat-app
    tag: latest
  replicas: 1
  ports:
    containerPort: 8501 # Application's container port
  env:
    CLIENT_ID: flask_client
    CLIENT_SECRET: fxAtVg6qe1eh78V4NurL3SeSNm2v8tUD
    KEYCLOAK_URL: https://keycloak.nebula.sl
    REALM: text2sql
    REDIRECT_URI: https://application-route-starchat.apps.nebula.sl
    OLLAMA_MODEL_NAME: llama3:instruct
    OLLAMA_MODEL_BASE_URL: http://model:11434
    VLLM_MODEL_NAME: Llama-3.1-8B-Instruct-AWQ
    VLLM_MODEL_BASE_URL: https://llama-8b-route-sy-vllm.apps.nebula.sl/v2/models
    VLLM_FULL_MODEL: https://triton-llama8b-route-triton-inference-services.apps.nebula.sl/v2/models/Meta-Llama-3-8B-Instruct-AWQ/generate
    LLM_PROVIDER: vllm
    LOGGING_URL: postgresql+psycopg2://user:pass@logging:5432/logging
    LOGGING: False

# Container resource settings (shared across applications)
resources:
  requests:
    cpu: "0.5"
    memory: "1Gi"
  limits:
    cpu: "1"
    memory: "2Gi"

# Route settings for "application" (OpenShift)
routeApplication:
  name: application-route
  tlsTermination: edge
  wildcardPolicy: None
